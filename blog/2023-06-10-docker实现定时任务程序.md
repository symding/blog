---
title: docker实现定时任务程序
tags: [python,docker]
---

> 基于 python 的 `apscheduler` 包来管理定时任务，使用 docker 来执行定时任务。
<!--truncate-->

### 背景
当前的爬虫系统任务种类繁多，不同业务、不同的页面都要求制定更加细致的爬虫任务队列管理，因此需要一个集中的任务队列管理平台，定时执行各类任务检查程序，确保各类业务的调用准时完成，同时能完成一些低负载的统计任务。

### 架构
* `mysql` 来存储定时任务的 `cron` 表达式以及具体业务代码
* 使用 python 来编写任务代码
* `apscheduler` 模块来管理定时任务，定时任务执行时通过docker生成容器，同时从 mysql 注入代码后完成任务执行

### 实现
#### python 执行字符串代码
使用 `exec()` 函数来将字符串类型的python代码转换成可执行代码
```python title="task_runner.py"
function_str = """def task():
print("hello world")
"""
exec(function_str + "task()")
```
能执行还不够，还需要对任务执行过程进行记录，生成日志，`exec` 函数可以通过参数来注入全局变量因此可以自定义一个日志类注入到任务中
```python title="task_runner.py"
class Logger():
    def __init__(self,task_name):
        self.info_str = ''
        self.error = ''
        self.task_name = task_name

    def info(self, log_data):
        self.info_str += datetime.datetime.now(tz=pytz.timezone(
            'Asia/Shanghai')).strftime('%Y-%m-%d %H:%M:%S') + ': ' + str(log_data) + "\n"

    # 任务出错时搜集错误堆栈以供调试
    def exception(self, log_data):
        self.error = str(log_data)
    
    # 保存相关日志数据
    def save(self):
        pass

# function_catch 将执行函数包裹起来，捕获其运行错误。
def function_catch(func, logger):
    import traceback
    try:
        func()
    except:
        logger.exception(traceback.format_exc())

def task_runner():
    function_str = """def task():
    print("hello world")
    """
    exec_str = f"{function_str}\n{function_catch(task)}"
    logger = Logger()
    exec(exec_str,{"function_catch":function_catch,"logger":logger})
```
在加上从数据库中取任务代码的逻辑，就完成了执行一个定时任务的完整流程
```python title="task_runner.py"
def task_runner(task_name):
    conn = db.Mysql
    cursor = conn.cursor()
    sql = "select taskFuncStr from scheduler_task_info where taskName=%s"
    cursor.execute(sql, (task_name,))
    function_str = cursor.fetchone()[0]
    exec_str = f"{function_str}\n{function_catch(task)}"
    logger = Logger()
    exec(exec_str,{"function_catch":function_catch,"logger":logger})
    logger.save()

if __name__ == "__main__":
    task_name = sys.argv[1]
    task_runner(task_name)
```
#### docker 执行定时任务
```python
def create_container(taskName):
    cron_log.warning(f'开始执行: {taskName}')
    try:
        client = docker.DockerClient()
        client.containers.run(
            name=taskName,
            image="包含task_runner.py的python环境",
            entrypoint=["python3", "task_runner.py", taskName],
            auto_remove=True,
            remove=True,
            detach=True
        )
        client.close()
        cron_log.warning(f"{taskName} 已启动.")
    except Exception as e:
        cron_log.error(f"启动 {taskName}发生错误: " +str(e))

```
#### `apscheduler` 调度任务
建立一个循环从数据库中读取已经激活的任务即可
```python title="scheduler.py"
def main():
    scheduler = BackgroundScheduler(timezone=timezone('Asia/Shanghai'))
    scheduler.start()
    while True:
        conn = db.Mysql
        cursor = conn.cursor()
        cursor.execute(
            "select taskName,taskCron,taskState from scheduler_task_info")
        for name, cron, state in cursor.fetchall():
            job = scheduler.get_job(name)
            if state == "Active":
                if job is not None:
                    new_trigger = scheduler._create_trigger("cron",cron_parse(cron))
                    running_trigger = job.trigger
                    if str(new_trigger) != str(running_trigger):
                        cron_log.warning(f'job {name} cron表达式已发生更改，需要重建。')
                        scheduler.remove_job(name)
                        job = None
                        cron_log.warning(f'job {name} 已移除。[state: {state}]')
                if job is None:
                    scheduler.add_job(
                    create_container,
                    id=name,
                    args=[name],
                    trigger='cron',
                    **cron_parse(cron))
                    cron_log.warning(f'已添加 {name}。[state: {state}]')
            elif state != "Active" and job is not None:
                scheduler.remove_job(name)
                cron_log.warning(f'job {name} 已移除。[state: {state}]')
        time.sleep(5)
        conn.close()
```